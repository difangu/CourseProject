{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "df_raw = spark.read.options(header='False',wholetext=True)\\\r\n",
        "    .text('adl://cp-bizops-c15.azuredatalakestore.net/local/users/lowen/UIUC/dblp.xml')\r\n",
        "#df_raw['value_1']=df_raw['value'].apply(lambda x: ' '.join(x) )\r\n",
        "\r\n",
        "#df_text=' '.join(df_raw['value_1'])\r\n",
        "#df_raw.write.mode(\"overwrite\").saveAsTable(\"default.dblp\")\r\n",
        "df_raw.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:34.1517224Z",
              "execution_start_time": "2020-12-07T02:25:03.7533433Z",
              "execution_finish_time": "2020-12-07T02:25:20.3631027Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 259,
          "data": {
            "text/plain": "+--------------------+\n|               value|\n+--------------------+\n|<?xml version=\"1....|\n|<!DOCTYPE dblp SY...|\n|              <dblp>|\n|<phdthesis mdate=...|\n|<author>Carmen He...|\n|<title>Modell zur...|\n|   <year>2010</year>|\n|<school>Aarhus Un...|\n|<pages>1-315</pages>|\n|<isbn>978-3-86596...|\n|<ee>http://d-nb.i...|\n|</phdthesis><phdt...|\n|<author>Gerd Hoff...|\n|<title>Ein Verfah...|\n|   <year>2002</year>|\n|<school>Universit...|\n|<ee>http://ubt.op...|\n|<ee>https://nbn-r...|\n|<ee>http://d-nb.i...|\n|        </phdthesis>|\n+--------------------+\nonly showing top 20 rows"
          },
          "metadata": {}
        }
      ],
      "execution_count": 259,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_raw.select('value').collect()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:34.3264886Z",
              "execution_start_time": "2020-12-07T02:25:20.3907308Z",
              "execution_finish_time": "2020-12-07T02:31:58.9605996Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 260,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 260,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mstr=\"\"\r\n",
        "flag=False\r\n",
        "dblp=[]\r\n",
        "for row in df:\r\n",
        "    s=str(row.value)\r\n",
        "    if s.__contains__('</phdthesis>'):\r\n",
        "        i=s.index('</phdthesis>')\r\n",
        "        s1=s[0:i+len('</phdthesis>')]\r\n",
        "        mstr=mstr+s1\r\n",
        "        dblp.append(mstr)\r\n",
        "        mstr=\"\"\r\n",
        "        flag=False\r\n",
        "    if s.__contains__('<phdthesis'):\r\n",
        "        i=s.index('<phdthesis')\r\n",
        "        mstr=mstr+s[i:]\r\n",
        "        flag=True\r\n",
        "    else:\r\n",
        "        if flag==True:\r\n",
        "            mstr=mstr+s"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:34.5290438Z",
              "execution_start_time": "2020-12-07T02:31:58.9859617Z",
              "execution_finish_time": "2020-12-07T02:34:51.9690555Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 261,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 261,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET \r\n",
        "list_dblp=[]\r\n",
        "for d in dblp:\r\n",
        "    dict_dblp={}\r\n",
        "    try:\r\n",
        "        tree = ET.fromstring(d.replace('&','')) \r\n",
        "        for child in tree:\r\n",
        "            #print(child.tag, ' '.join(child.itertext()))\r\n",
        "            dict_dblp[child.tag]=' '.join(child.itertext())\r\n",
        "        list_dblp.append(dict_dblp)\r\n",
        "    except :\r\n",
        "        print(d)\r\n",
        "\r\n",
        "list_dblp[:2]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:34.7202748Z",
              "execution_start_time": "2020-12-07T02:34:51.99468Z",
              "execution_finish_time": "2020-12-07T02:34:54.0359837Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 262,
          "data": {
            "text/plain": "[{'author': 'Carmen Heine', 'title': 'Modell zur Produktion von Online-Hilfen.', 'year': '2010', 'school': 'Aarhus University', 'pages': '1-315', 'isbn': '978-3-86596-263-8', 'ee': 'http://d-nb.info/996064095'}, {'author': 'Gerd Hoff', 'title': 'Ein Verfahren zur thematisch spezialisierten Suche im Web und seine Realisierung im Prototypen HomePageSearch', 'year': '2002', 'school': 'University of Trier, Germany', 'ee': 'http://d-nb.info/971713243'}]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 262,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET \r\n",
        "list_dblp_together=[]\r\n",
        "for d in dblp:\r\n",
        "    dict_dblp={}\r\n",
        "    try:\r\n",
        "        tree = ET.fromstring(d.replace('&','')) \r\n",
        "        #dict_dblp[child.tag]=' '.join(child.itertext())\r\n",
        "        list_dblp_together.append(' '.join(tree.itertext()))\r\n",
        "    except :\r\n",
        "        print(d)\r\n",
        "\r\n",
        "list_dblp_together[:2]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:34.8289132Z",
              "execution_start_time": "2020-12-07T02:34:54.0642681Z",
              "execution_finish_time": "2020-12-07T02:34:56.100104Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 263,
          "data": {
            "text/plain": "['Carmen Heine Modell zur Produktion von Online-Hilfen. 2010 Aarhus University 1-315 978-3-86596-263-8 http://d-nb.info/996064095', 'Gerd Hoff Ein Verfahren zur thematisch spezialisierten Suche im Web und seine Realisierung im Prototypen HomePageSearch 2002 University of Trier, Germany http://ubt.opus.hbz-nrw.de/volltexte/2004/146/ https://nbn-resolving.org/urn:nbn:de:hbz:385-1468 http://d-nb.info/971713243']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 263,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"value\"]\r\n",
        "\r\n",
        "df_all = spark.createDataFrame(data=[[p] for p in list_dblp_together],schema =columns)\r\n",
        "df_all.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"overwrite\").save('adl://cp-bizops-c15.azuredatalakestore.net/local/users/lowen/UIUC/dblp_all.txt')\r\n",
        "\r\n",
        "df_sample = spark.createDataFrame(data=[[p] for p in list_dblp_together[:500]],schema =columns)\r\n",
        "df_sample.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"overwrite\").save('adl://cp-bizops-c15.azuredatalakestore.net/local/users/lowen/UIUC/dblp_sample.txt')\r\n",
        "\r\n",
        "print(len(list_dblp),len(list_dblp_together),df_all.count(),df_sample.count())"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:34.9124461Z",
              "execution_start_time": "2020-12-07T02:34:56.1296217Z",
              "execution_finish_time": "2020-12-07T02:35:16.4737553Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 264,
          "data": {
            "text/plain": "78704 78704 78704 500"
          },
          "metadata": {}
        }
      ],
      "execution_count": 264,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\r\n",
        "from gensim import corpora\r\n",
        "from pprint import pprint\r\n",
        "###1. get the word list\r\n",
        "\r\n",
        "list_dblp_together[:2]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:35.0256672Z",
              "execution_start_time": "2020-12-07T02:35:16.5089075Z",
              "execution_finish_time": "2020-12-07T02:35:20.5662575Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 265,
          "data": {
            "text/plain": "['Carmen Heine Modell zur Produktion von Online-Hilfen. 2010 Aarhus University 1-315 978-3-86596-263-8 http://d-nb.info/996064095', 'Gerd Hoff Ein Verfahren zur thematisch spezialisierten Suche im Web und seine Realisierung im Prototypen HomePageSearch 2002 University of Trier, Germany http://ubt.opus.hbz-nrw.de/volltexte/2004/146/ https://nbn-resolving.org/urn:nbn:de:hbz:385-1468 http://d-nb.info/971713243']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 265,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [doc.split() for doc in list_dblp_together]\r\n",
        "\r\n",
        "texts[:2]\r\n",
        "dictionary = corpora.Dictionary(texts)\r\n",
        "print(dictionary)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:35.1185519Z",
              "execution_start_time": "2020-12-07T02:35:20.5910166Z",
              "execution_finish_time": "2020-12-07T02:35:26.713274Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 266,
          "data": {
            "text/plain": "Dictionary(358113 unique tokens: ['1-315', '2010', '978-3-86596-263-8', 'Aarhus', 'Carmen']...)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 266,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\r\n",
        "tokenized_list = [simple_preprocess(doc) for doc in list_dblp_together]\r\n",
        "##remove words with length<=3\r\n",
        "tokenized_list=[[w for w in s if len(w)>3]  for s in tokenized_list ]\r\n",
        "##stop words\r\n",
        "stopword=['http','https','university', 'info']\r\n",
        "tokenized_list=[[w for w in s if w not in stopword]  for s in tokenized_list ]\r\n",
        "\r\n",
        "dct = corpora.Dictionary(tokenized_list)\r\n",
        "\r\n",
        "#corpus = [dct.doc2bow(line) for line in tokenized_list]\r\n",
        "#corpus[:1]\r\n",
        "\r\n",
        "dct"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:35.2089204Z",
              "execution_start_time": "2020-12-07T02:35:26.7395638Z",
              "execution_finish_time": "2020-12-07T02:35:34.873635Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 267,
          "data": {
            "text/plain": "<gensim.corpora.dictionary.Dictionary object at 0x7f92eeb6d278>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 267,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###1. get the dictionary for all data\r\n",
        "from collections import Counter\r\n",
        "##1.1. get the (word, count) pair dictionary\r\n",
        "dictionary={}\r\n",
        "\r\n",
        "for s in tokenized_list:\r\n",
        "    for w in s:\r\n",
        "        dictionary[w]=dictionary.get(w,0)+1\r\n",
        "##1.2. sorting to get the top 500 words\r\n",
        "top_words=Counter(dictionary)\r\n",
        "top_words=top_words.most_common(200)\r\n",
        "#topwords_dict\r\n",
        "top_words=[w[0] for w in top_words]\r\n",
        "###2. deal with the two words phrase\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:35.3646914Z",
              "execution_start_time": "2020-12-07T02:35:34.8994604Z",
              "execution_finish_time": "2020-12-07T02:35:36.9376743Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 268,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 268,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict={}\r\n",
        "i=1\r\n",
        "for word in top_words:\r\n",
        "    word_dict[word]=i\r\n",
        "    i=i+1\r\n",
        "#print(word_dict.get('germany'))\r\n",
        "word_dict"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:35.4963506Z",
              "execution_start_time": "2020-12-07T02:35:36.962906Z",
              "execution_finish_time": "2020-12-07T02:35:38.9913524Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 269,
          "data": {
            "text/plain": "{'ethos': 1, 'search': 2, 'base': 3, 'germany': 4, 'ndltd': 5, 'handle': 6, 'archives': 7, 'ouvertes': 8, 'library': 9, 'france': 10, 'orderdetails': 11, 'british': 12, 'record': 13, 'technology': 14, 'resolving': 15, 'ibict': 16, 'institute': 17, 'systems': 18, 'agregador': 19, 'repositorio': 20, 'union': 21, 'based': 22, 'show': 23, 'pour': 24, 'fuuml': 25, 'eacute': 26, 'brazil': 27, 'opus': 28, 'teses': 29, 'agrave': 30, 'data': 31, 'analysis': 32, 'networks': 33, 'with': 34, 'atilde': 35, 'reacute': 36, 'dspace': 37, 'application': 38, 'applications': 39, 'technical': 40, 'verlag': 41, 'design': 42, 'learning': 43, 'volltexte': 44, 'cambridge': 45, 'using': 46, 'multi': 47, 'system': 48, 'paris': 49, 'berlin': 50, 'munich': 51, 'london': 52, 'information': 53, 'dans': 54, 'software': 55, 'grenoble': 56, 'deacute': 57, 'massachusetts': 58, 'unicamp': 59, 'college': 60, 'model': 61, 'meacute': 62, 'item': 63, 'reposip': 64, 'models': 65, 'index': 66, 'eprints': 67, 'approach': 68, 'diss': 69, 'management': 70, 'algorithms': 71, 'systegrave': 72, 'karlsruhe': 73, 'analyse': 74, 'simulation': 75, 'from': 76, 'time': 77, 'methods': 78, 'california': 79, 'escholarship': 80, 'ftcdlib': 81, 'rwth': 82, 'darmstadt': 83, 'aachen': 84, 'control': 85, 'html': 86, 'distributed': 87, 'modeling': 88, 'computer': 89, 'eines': 90, 'performance': 91, 'federal': 92, 'techniques': 93, 'satilde': 94, 'image': 95, 'network': 96, 'saarland': 97, 'paulo': 98, 'dresden': 99, 'shaker': 100, 'diva': 101, 'framework': 102, 'bdtd_usp': 103, 'development': 104, 'efficient': 105, 'wireless': 106, 'ufsc': 107, 'detection': 108, 'optimization': 109, 'kobv': 110, 'dynamic': 111, 'processing': 112, 'images': 113, 'para': 114, 'seacute': 115, 'unsw': 116, 'mobile': 117, 'recognition': 118, 'stuttgart': 119, 'modelling': 120, 'michael': 121, 'thomas': 122, 'entwicklung': 123, 'high': 124, 'modegrave': 125, 'lisation': 126, 'architecture': 127, 'modeacute': 128, 'tede': 129, 'communication': 130, 'georgia': 131, 'arizona': 132, 'machine': 133, 'evaluation': 134, 'seaux': 135, 'digital': 136, 'real': 137, 'sweden': 138, 'ri_unicamp': 139, 'joseph': 140, 'teacute': 141, 'unter': 142, 'programming': 143, 'knowledge': 144, 'scidok': 145, 'service': 146, 'computing': 147, 'atlanta': 148, 'ftgeorgiatech': 149, 'smartech': 150, 'gatech': 151, 'tuprints': 152, 'security': 153, 'technische': 154, 'state': 155, 'study': 156, 'adaptive': 157, 'australia': 158, 'geacute': 159, 'parallel': 160, 'fourier': 161, 'uuid': 162, 'erlangen': 163, 'hochschule': 164, 'human': 165, 'automatic': 166, 'language': 167, 'disponiveis': 168, 'cole': 169, 'theses': 170, 'martin': 171, 'optimisation': 172, 'interaction': 173, 'large': 174, 'donneacute': 175, 'einer': 176, 'eine': 177, 'problems': 178, 'neural': 179, 'jspui': 180, 'support': 181, 'environments': 182, 'estimation': 183, 'user': 184, 'object': 185, 'visual': 186, 'architectures': 187, 'classification': 188, 'towards': 189, 'peter': 190, 'edinburgh': 191, 'etheses': 192, 'park': 193, 'hamburg': 194, 'leacute': 195, 'lyon': 196, 'internet': 197, 'rennes': 198, 'services': 199, 'nuremberg': 200}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 269,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###2.1 get all top words in each sentens\r\n",
        "for s in tokenized_list[:5]:\r\n",
        "    word_group=[]\r\n",
        "    for w in top_words:\r\n",
        "        if w in s:\r\n",
        "            word_group.append(w)\r\n",
        "    print(word_group)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:35.6049266Z",
              "execution_start_time": "2020-12-07T02:35:39.0258473Z",
              "execution_finish_time": "2020-12-07T02:35:41.0518777Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 270,
          "data": {
            "text/plain": "[]\n['germany', 'resolving', 'opus', 'volltexte']\n['system', 'california', 'performance', 'support']\n['resolving', 'opus', 'networks', 'volltexte', 'mobile', 'problems', 'support', 'towards']\n['applications', 'evaluation']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 270,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2 get the two words phrase\r\n",
        "top_words_temp=top_words.copy()\r\n",
        "for s in tokenized_list[:3]:\r\n",
        "    word_group=[]\r\n",
        "    for w in top_words:\r\n",
        "        word_first=''\r\n",
        "        if w in s:\r\n",
        "            word_first=w\r\n",
        "            word_group.append(w)\r\n",
        "            word_group.append('-1')\r\n",
        "            #print(word_first)\r\n",
        "            top_words_temp.remove(w)\r\n",
        "            for w1 in top_words_temp:\r\n",
        "                if w1 in s:\r\n",
        "                    word_group.append(word_first+' ' +w1)\r\n",
        "                    word_group.append('-1')\r\n",
        "    word_group.append('-2')\r\n",
        "    print(word_group)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:35.7213672Z",
              "execution_start_time": "2020-12-07T02:35:41.0816686Z",
              "execution_finish_time": "2020-12-07T02:35:43.1163429Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 271,
          "data": {
            "text/plain": "['-2']\n['germany', '-1', 'germany resolving', '-1', 'germany opus', '-1', 'germany volltexte', '-1', 'resolving', '-1', 'resolving opus', '-1', 'resolving volltexte', '-1', 'opus', '-1', 'opus volltexte', '-1', 'volltexte', '-1', '-2']\n['system', '-1', 'system california', '-1', 'system performance', '-1', 'system support', '-1', 'california', '-1', 'california performance', '-1', 'california support', '-1', 'performance', '-1', 'performance support', '-1', 'support', '-1', '-2']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 271,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###2.3 output\r\n",
        "output=[]\r\n",
        "for s in tokenized_list:\r\n",
        "    top_words_temp=top_words.copy()\r\n",
        "    word_group=[]\r\n",
        "    for w in top_words:\r\n",
        "        word_first=''\r\n",
        "        if w in s:\r\n",
        "            word_first=w\r\n",
        "            word_group.append(str(word_dict.get(w)))\r\n",
        "            word_group.append('-1')\r\n",
        "            #print(word_first)\r\n",
        "            if w in top_words_temp:\r\n",
        "                top_words_temp.remove(w)\r\n",
        "            else:\r\n",
        "                print(w)\r\n",
        "            for w1 in top_words_temp:\r\n",
        "                if w1 in s:\r\n",
        "                    word_group.append(str(word_dict.get(word_first))+' ' +str(word_dict.get(w1)))\r\n",
        "                    word_group.append('-1')\r\n",
        "    word_group.append('-2')\r\n",
        "    #print(word_group)\r\n",
        "    l=''\r\n",
        "    for s in word_group:\r\n",
        "        l=l+' '+s\r\n",
        "    l=l[1:]\r\n",
        "    if l!='-2':\r\n",
        "        output.append(l)\r\n",
        "\r\n",
        "#print(output)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:39:06.0410364Z",
              "execution_start_time": "2020-12-07T02:39:06.0750993Z",
              "execution_finish_time": "2020-12-07T02:39:54.9673765Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 26, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 282,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 282,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)\r\n",
        "#[' -2', ' 4 -1 4 15 -1 4 28 -1 4 44 -1 15 -1 15 28 -1 15 44 -1 28 -1 28 44 -1 44 -1 -2', ' 48 -1 48 79 -1 48 91 -1 48 181 -1 79 -1 79 91 -1 79 181 -1 91 -1 91 181 -1 181 -1 -2']"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 22,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:37:27.5910627Z",
              "execution_start_time": "2020-12-07T02:37:27.6190397Z",
              "execution_finish_time": "2020-12-07T02:37:29.6430493Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 22, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 278,
          "data": {
            "text/plain": "[' -2', ' 4 -1 4 15 -1 4 28 -1 4 44 -1 15 -1 15 28 -1 15 44 -1 28 -1 28 44 -1 44 -1 -2', ' 48 -1 48 79 -1 48 91 -1 48 181 -1 79 -1 79 91 -1 79 181 -1 91 -1 91 181 -1 181 -1 -2']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 278,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_all = spark.createDataFrame(data=[[p] for p in output],schema =columns)\r\n",
        "output_all.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"overwrite\").save('adl://cp-bizops-c15.azuredatalakestore.net/local/users/lowen/UIUC/dblp_output.txt')\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:39:34.5226269Z",
              "execution_start_time": "2020-12-07T02:39:54.9964671Z",
              "execution_finish_time": "2020-12-07T02:40:21.4345486Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 27, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 283,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 283,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_list[:3]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 18,
              "state": "submitted",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:36.0257062Z",
              "execution_start_time": "2020-12-07T02:36:36.0433198Z",
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 18, Submitted, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 274,
          "data": {
            "text/plain": "[['carmen', 'heine', 'modell', 'produktion', 'online', 'hilfen', 'aarhus'], ['gerd', 'hoff', 'verfahren', 'thematisch', 'spezialisierten', 'suche', 'seine', 'realisierung', 'prototypen', 'homepagesearch', 'trier', 'germany', 'opus', 'volltexte', 'resolving'], ['margo', 'seltzer', 'file', 'system', 'performance', 'transaction', 'support', 'california', 'berkeley', 'berkeley', 'papers']]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 274,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\r\n",
        "from gensim import corpora\r\n",
        "from pprint import pprint\r\n",
        "\r\n",
        "# How to create a dictionary from a list of sentences?\r\n",
        "documents = [\"The Saudis are preparing a report that will acknowledge that\", \r\n",
        "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \r\n",
        "             \"interrogation that went wrong, one that was intended to lead\", \r\n",
        "             \"to his abduction from Turkey, according to two sources.\"]\r\n",
        "\r\n",
        "documents_2 = [\"One source says the report will likely conclude that\", \r\n",
        "                \"the operation was carried out without clearance and\", \r\n",
        "                \"transparency and that those involved will be held\", \r\n",
        "                \"responsible. One of the sources acknowledged that the\", \r\n",
        "                \"report is still being prepared and cautioned that\", \r\n",
        "                \"things could change.\"]\r\n",
        "\r\n",
        "# Tokenize(split) the sentences into words\r\n",
        "texts = [[text for text in doc.split()] for doc in documents]\r\n",
        "texts"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:36.1430477Z",
              "execution_start_time": "2020-12-07T02:36:36.0681219Z",
              "execution_finish_time": "2020-12-07T02:36:38.1043952Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 19, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 275,
          "data": {
            "text/plain": "[['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], ['Saudi', 'journalist', 'Jamal', \"Khashoggi's\", 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong,', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'Turkey,', 'according', 'to', 'two', 'sources.']]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 275,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for s in list_dblp_together[:2]:\r\n",
        "    s1=word_tokenize(s)\r\n",
        "    print(s1)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool3",
              "session_id": 22,
              "statement_id": 20,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-12-07T02:21:36.278486Z",
              "execution_start_time": "2020-12-07T02:36:38.127979Z",
              "execution_finish_time": "2020-12-07T02:36:40.1645809Z"
            },
            "text/plain": "StatementMeta(sparkpool3, 22, 20, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'word_tokenize' is not defined",
          "traceback": [
            "NameError: name 'word_tokenize' is not defined",
            "Traceback (most recent call last):\n",
            "NameError: name 'word_tokenize' is not defined\n"
          ]
        }
      ],
      "execution_count": 276,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd \r\n",
        "#df_dblp=pd.DataFrame.from_dict(list_dblp[:1])\r\n",
        "#display(df_dblp)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2020-12-07T02:21:36.3647207Z",
              "execution_start_time": null,
              "execution_finish_time": "2020-12-07T02:36:40.1656528Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "sessionOptions": {
      "driverMemory": "56g",
      "driverCores": 8,
      "executorMemory": "56g",
      "executorCores": 8,
      "numExecutors": 3,
      "keepAliveTimeout": 30,
      "conf": {
        "spark.dynamicAllocation.enabled": "false",
        "spark.dynamicAllocation.minExecutors": "3",
        "spark.dynamicAllocation.maxExecutors": "3"
      }
    },
    "saveOutput": true,
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "a365ComputeOptions": {
      "nodeSize": "Medium",
      "auth": {
        "authResource": "https://dev.azuresynapse.net",
        "type": "AAD"
      },
      "name": "sparkpool3",
      "nodeCount": 10,
      "endpoint": "https://dsasa.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool3",
      "automaticScaleJobs": false,
      "type": "Spark",
      "id": "/subscriptions/523020eb-954d-4d10-b0fa-9b2e70996e93/resourceGroups/DataSolution/providers/Microsoft.Synapse/workspaces/dsasa/bigDataPools/sparkpool3",
      "sparkVersion": "2.4",
      "extraHeader": {}
    },
    "microsoft": {
      "language": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}